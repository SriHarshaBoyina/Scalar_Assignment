[
  {
    "id": "12312573",
    "key": "HADOOP-8",
    "title": "NDFS DataNode advertises localhost as it's address",
    "project": "HADOOP",
    "reporter": "Peter Sandstr\u00f6m",
    "assignee": null,
    "status": "Closed",
    "priority": "Major",
    "labels": [],
    "created": "2005-07-24T23:46:18+00:00",
    "updated": "2015-05-18T04:15:07+00:00",
    "description": "",
    "comments": [
      {
        "id": "12316614",
        "author": "Peter Sandstr\u00f6m",
        "created": "2005-07-24T23:51:00+00:00",
        "body": "fixes the problem by connecting to the NameNode and using the address that the local socket is bound to instead of calling getLocalHost(), beware that this makes machineName in the DataNode constructor unused."
      },
      {
        "id": "12367595",
        "author": "Doug Cutting",
        "created": "2006-02-24T08:00:04+00:00",
        "body": "The problem with this fix is that, if the namenode is down, the socket creation fails and we fail to launch a datanode.  So it would be great to have a better method than InetAddress.getLocalHost(), that tries harder to return the appropriate IP to advertise to the namenode, but that doesn't require the namenode to be up.\n\nPerhaps this could use NetworkInterfaces.getInetAddresses() and find the first that is not a loopback interface?"
      },
      {
        "id": "12367598",
        "author": "Doug Cutting",
        "created": "2006-02-24T08:13:30+00:00",
        "body": "This is a known JVM bug:\n\nhttp://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4665037\n\nSo let's let Sun fix it, rather than fix it ourselves.  One can workaround it by altering one's linux configuration."
      }
    ],
    "content": "NDFS DataNode advertises localhost as it's address\n\n\n\nfixes the problem by connecting to the NameNode and using the address that the local socket is bound to instead of calling getLocalHost(), beware that this makes machineName in the DataNode constructor unused.\n\nThe problem with this fix is that, if the namenode is down, the socket creation fails and we fail to launch a datanode.  So it would be great to have a better method than InetAddress.getLocalHost(), that tries harder to return the appropriate IP to advertise to the namenode, but that doesn't require the namenode to be up.\n\nPerhaps this could use NetworkInterfaces.getInetAddresses() and find the first that is not a loopback interface?\n\nThis is a known JVM bug:\n\nhttp://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4665037\n\nSo let's let Sun fix it, rather than fix it ourselves.  One can workaround it by altering one's linux configuration.",
    "derived": {
      "summary_prompt": "Summarize the following Jira issue:\n\nNDFS DataNode advertises localhost as it's address\n\n\n\nfixes the problem by connecting to the NameNode and using the address that the local socket is bound to instead of calling getLocalHost(), beware that this makes machineName in the DataNode constructor unused.\n\nThe problem with this fix is that, if the namenode is down, the socket creation fails and we fail to launch a datanode.  So it would be great to have a better method than InetAddress.getLocalHost(), that tries harder to return the appropriate IP to advertise to the namenode, but that doesn't require the namenode to be up.\n\nPerhaps this could use NetworkInterfaces.getInetAddresses() and find the first that is not a loopback interface?\n\nThis is a known JVM bug:\n\nhttp://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4665037\n\nSo let's let Sun fix it, rather than fix it ourselves.  One can workaround it by altering one's linux configuration.",
      "qa_prompt": "Write 3 question-answer pairs that help understand this issue:\n\nNDFS DataNode advertises localhost as it's address\n\n\n\nfixes the problem by connecting to the NameNode and using the address that the local socket is bound to instead of calling getLocalHost(), beware that this makes machineName in the DataNode constructor unused.\n\nThe problem with this fix is that, if the namenode is down, the socket creation fails and we fail to launch a datanode.  So it would be great to have a better method than InetAddress.getLocalHost(), that tries harder to return the appropriate IP to advertise to the namenode, but that doesn't require the namenode to be up.\n\nPerhaps this could use NetworkInterfaces.getInetAddresses() and find the first that is not a loopback interface?\n\nThis is a known JVM bug:\n\nhttp://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4665037\n\nSo let's let Sun fix it, rather than fix it ourselves.  One can workaround it by altering one's linux configuration."
    }
  },
  {
    "id": "12317701",
    "key": "HADOOP-19",
    "title": "Datanode corruption",
    "project": "HADOOP",
    "reporter": "Rod Taylor",
    "assignee": "Doug Cutting",
    "status": "Closed",
    "priority": "Critical",
    "labels": [],
    "created": "2005-10-07T11:46:42+00:00",
    "updated": "2009-07-08T16:41:48+00:00",
    "description": "Our admins accidentally started a second nutch datanode pointing to the same directories as one already running (same machine) which in turn caused the entire contents of the datanode to go disappear.\n\nThis happened because the blocking was based on the username (since fixed in our start scripts) and it was started as two different users.\n\nThe ndfs.name.dir and ndfs.data.dir directories were both completely devoid of content, where they had about 150GB not all that much earlier.\n\n\nI think the solution is improved interlocking within the data directory itself (file locked with flock or something similar).",
    "comments": [
      {
        "id": "12372133",
        "author": "Doug Cutting",
        "created": "2006-03-29T03:15:58+00:00",
        "body": "I added a format command to namenode.  Now one must run 'bin/hadoop namenode -format' whenever a new dfs.data.dir is configured.  A misconfigured namenode will thus no longer create a new empty filesystem and then tell all datanodes to discard their blocks."
      }
    ],
    "content": "Datanode corruption\n\nOur admins accidentally started a second nutch datanode pointing to the same directories as one already running (same machine) which in turn caused the entire contents of the datanode to go disappear.\n\nThis happened because the blocking was based on the username (since fixed in our start scripts) and it was started as two different users.\n\nThe ndfs.name.dir and ndfs.data.dir directories were both completely devoid of content, where they had about 150GB not all that much earlier.\n\n\nI think the solution is improved interlocking within the data directory itself (file locked with flock or something similar).\n\nI added a format command to namenode.  Now one must run 'bin/hadoop namenode -format' whenever a new dfs.data.dir is configured.  A misconfigured namenode will thus no longer create a new empty filesystem and then tell all datanodes to discard their blocks.",
    "derived": {
      "summary_prompt": "Summarize the following Jira issue:\n\nDatanode corruption\n\nOur admins accidentally started a second nutch datanode pointing to the same directories as one already running (same machine) which in turn caused the entire contents of the datanode to go disappear.\n\nThis happened because the blocking was based on the username (since fixed in our start scripts) and it was started as two different users.\n\nThe ndfs.name.dir and ndfs.data.dir directories were both completely devoid of content, where they had about 150GB not all that much earlier.\n\n\nI think the solution is improved interlocking within the data directory itself (file locked with flock or something similar).\n\nI added a format command to namenode.  Now one must run 'bin/hadoop namenode -format' whenever a new dfs.data.dir is configured.  A misconfigured namenode will thus no longer create a new empty filesystem and then tell all datanodes to discard their blocks.",
      "qa_prompt": "Write 3 question-answer pairs that help understand this issue:\n\nDatanode corruption\n\nOur admins accidentally started a second nutch datanode pointing to the same directories as one already running (same machine) which in turn caused the entire contents of the datanode to go disappear.\n\nThis happened because the blocking was based on the username (since fixed in our start scripts) and it was started as two different users.\n\nThe ndfs.name.dir and ndfs.data.dir directories were both completely devoid of content, where they had about 150GB not all that much earlier.\n\n\nI think the solution is improved interlocking within the data directory itself (file locked with flock or something similar).\n\nI added a format command to namenode.  Now one must run 'bin/hadoop namenode -format' whenever a new dfs.data.dir is configured.  A misconfigured namenode will thus no longer create a new empty filesystem and then tell all datanodes to discard their blocks."
    }
  },
  {
    "id": "12317942",
    "key": "HADOOP-10",
    "title": "ndfs.replication is not documented within the nutch-default.xml configuration file.",
    "project": "HADOOP",
    "reporter": "Rod Taylor",
    "assignee": null,
    "status": "Closed",
    "priority": "Trivial",
    "labels": [],
    "created": "2005-10-14T06:02:45+00:00",
    "updated": "2009-07-08T16:41:47+00:00",
    "description": "ndfs.replication is not documented within the nutch-default.xml configuration file.",
    "comments": [
      {
        "id": "12365319",
        "author": "Doug Cutting",
        "created": "2006-02-07T04:15:25+00:00",
        "body": "This has been fixed.  The paramter is now dfs.replication in hadoop-default.xml."
      }
    ],
    "content": "ndfs.replication is not documented within the nutch-default.xml configuration file.\n\nndfs.replication is not documented within the nutch-default.xml configuration file.\n\nThis has been fixed.  The paramter is now dfs.replication in hadoop-default.xml.",
    "derived": {
      "summary_prompt": "Summarize the following Jira issue:\n\nndfs.replication is not documented within the nutch-default.xml configuration file.\n\nndfs.replication is not documented within the nutch-default.xml configuration file.\n\nThis has been fixed.  The paramter is now dfs.replication in hadoop-default.xml.",
      "qa_prompt": "Write 3 question-answer pairs that help understand this issue:\n\nndfs.replication is not documented within the nutch-default.xml configuration file.\n\nndfs.replication is not documented within the nutch-default.xml configuration file.\n\nThis has been fixed.  The paramter is now dfs.replication in hadoop-default.xml."
    }
  }
]